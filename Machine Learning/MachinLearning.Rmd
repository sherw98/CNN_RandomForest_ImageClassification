---
title: "Final Project"
author: "Andrew Tiu, Kelly Wu, Yixuan (Sherry) Wu"
date: "April 30, 2020"
header-includes:
   - \usepackage{indentfirst}
output:
    bookdown::pdf_book:
      toc: true
      fig_caption: true
      number_section: true
    
fontsize: 11pt
float: true
mainfont: Times New Roman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, fig.height = 2.5, fig.width = 7, fig.align = "center", message = FALSE, warning = F, collapse = T)
library(tidyverse)
library(imager)
library(randomForest)
library(cluster)
library(clusternor)
library(boot)
library(caret)
library(gtools)
library(MASS)
library(mlbench)
library(kernlab)
library(e1071)   
library(knitr)
library(ranger)
library(dplyr)
```

#Data Read-in & Clean-up

##Training set

First, read in all the train files.
Each `files_{name}[i]` will display a file route directed to one photo. 

```{r readin}
##setup code
files_buildings = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_train/buildings", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T, )))
files_forest = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_train/forest", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
files_glacier = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_train/glacier",
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
files_mountain = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_train/mountain", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
files_sea = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_train/sea", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
files_street = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_train/street", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
```

Since our laptops are unable to handle all the photos, we decided to randomly select 200 photos from each categoryâ€™s training set. 

```{r}
##randomly selected 200 photos from each category (with set.seed(1)). 
set.seed(1)
f_buildings = sample(files_buildings, 200)
f_forest = sample(files_forest, 200)
f_glacier = sample(files_glacier, 200)
f_mountain = sample(files_mountain, 200)
f_sea = sample(files_sea, 200)
f_street = sample(files_street, 200)
```

```{r}
##combine all six files and load the image
files = c(f_buildings, f_forest, f_glacier, f_mountain, f_sea, f_street)
image_list = lapply(files, load.image)
image_list[[1]] #what the info of the first   image looks like

##get the classname from the directory 
class_list = basename(dirname(files))
table(class_list)

remove(f_buildings, f_forest, f_glacier, f_mountain, f_sea, f_street, 
       files_buildings, files_forest, files_glacier, 
       files_mountain, files_sea, files_street, files)
```

For each `i` in `image_list[[i]]`, `image_list[[i]]` contains info of one image. To expand these info/pixels, we need to transform each of them to a vector. Use `img_matrix` to set up the empty matirx with numbers of columns to be 67500 (150*150). For each image, the value of each pixel will be stored in each of the columns. 

Use `badimage` to count for all images that are not 150*150 pixels, and these images will be discarded from further analyses. 

Use the `for` loop to change each list to a vector. Details in comments. Use `img_df` to create a new data frame from the matrix. 

```{r}
#build the structure of an empty matrix
img_matrix = matrix(ncol = 67500, nrow = length(image_list))
badimage = NULL 

for(i in 1:length(image_list)){
  if(length(as.vector(image_list[[i]])) == 67500) {
    img_matrix[i,] = c(as.vector(image_list[[i]]))
    #fill in each row with one observation's pixels
    #do this for each observation unless photo does not meet requirement
  }
  else{badimage = append(badimage, i)} 
  #if image does not meet requirement, record which i it is 
  #so when appending class to each observation, 
  #will use this badimage to get rid of the class of those not met the requirement
}

img_df = as.data.frame(img_matrix[-badimage,])
print(dim(img_df)) #dimension of the training set
```

The number of rows is not exactly 1200 because there are several images that do not meet the 150*150 size criteria, so they were discarded from further analysis. 

Add class assignment to the data frame

```{r}
#append class and remove excessive values
new_class = class_list[-badimage]
img_df$class = as.factor(new_class)

remove(img_matrix)
```
##Test Data

The procedure should be similar. 

```{r}
#everything the same for test data
files_buildings = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_test/buildings", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
files_forest = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_test/forest", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
files_glacier = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_test/glacier", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
files_mountain = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_test/mountain", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
files_sea = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_test/sea", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
files_street = mixedsort(
  sort(
    list.files("~/Data Science/Project/seg_test/street", 
               pattern = "*.jpg", all.files = T, full.names = T, no.. = T)))
```

```{r}
f_buildings = sample(files_buildings, 200)
f_forest = sample(files_forest, 200)
f_glacier = sample(files_glacier, 200)
f_mountain = sample(files_mountain, 200)
f_sea = sample(files_sea, 200)
f_street = sample(files_street, 200)
```

```{r}
files = c(f_buildings, f_forest, f_glacier, f_mountain, f_sea, f_street)
image_list = lapply(files, load.image)
##class
class_list = basename(dirname(files))
remove(f_buildings, f_forest, f_glacier, f_mountain, f_sea, f_street, 
       files_buildings, files_forest, files_glacier, 
       files_mountain, files_sea, files_street, files)
```

```{r}
img_matrix = matrix(ncol = 67500, nrow = length(image_list))
badimage = NULL 

for(i in 1:length(image_list)){
  if(length(as.vector(image_list[[i]])) == 67500) {
    img_matrix[i,] = c(as.vector(image_list[[i]]))
    #fill in each row with one observation's pixels
    #do this for each observation unless photo does not meet requirement
  }
  else{badimage = append(badimage, i)} 
  #if image does not meet requirement, record which i it is 
  #so when appending class to each observation, 
  #will use this badimage to get rid of the class of those not met the requirement
}

test_df = as.data.frame(img_matrix[-badimage,])

new_class = class_list[-badimage]
test_df$class = as.factor(new_class)

remove(img_matrix, image_list)
```

#PCA

##PCA on the training set

Run the `prcomp` command to construct the PCA

```{r pca}
colclass = ncol(img_df)
img_pca = prcomp(img_df[, -colclass], center = T, scale = T)
```

See how much varaince each PCs explains, so first by computing the cumulative sum of variance explained, and output the number of PCs that expalins 75, 80, and 90 percent of total variance. THe `img_cumsum` contians the variance each pc explains. Each `pc??` is the index that contains variance closet to 75, 80, or 90 percent.  

```{r}
#get the cumulative variance explained
pr.var = img_pca$sdev^2 
pve=pr.var/sum(pr.var)

img_cumsum = cumsum(pve) 
img_cumsum[1:10]

pc75 = which.min(abs(img_cumsum - 0.75))
pc80 = which.min(abs(img_cumsum - 0.80))
pc90 = which.min(abs(img_cumsum - 0.90))
pc75
pc80
pc90
```

Build the training set data frame based on PCA results

```{r}
#build dataframe
df75 = data.frame(img_df$class, img_pca$x[,1:pc75])
colnames(df75)[1] = "class"

print(dim(df75) )#dimension on the train dataset
```

##On testing set

```{r}
#project the pcs onto the test data
pcatest = predict(img_pca, newdata = test_df)
pcatest = as.data.frame(pcatest)

#format the test data
test75 = pcatest[, 1:pc75]
test75 = data.frame(test_df$class, test75)
colnames(test75)[1] = "class"
dim(test75) # dimension of the new test data
```

Therefore, we now have the projected data frame.


#kNN

Normalize the variables

```{r}
#normalizing function
normalize <- function(x) {
return ((as.numeric(x) - min(as.numeric(x))) / (max(as.numeric(x)) - min(as.numeric(x)))) }

#normalize the train dataset followed by the test dataset
normdf = apply(df75[, -1], 2, normalize)
normdf = data.frame(df75$class, normdf)
colnames(normdf)[1] = "class"
normdf$class = as.factor(normdf$class)
normtest = apply(test75[,-1], 2, normalize)
normtest = data.frame(test75$class, normtest)
colnames(normtest)[1] = "class"
```

Use 10-fold CV to assess the optimal number of neighbors on the entire data (training + testing)

```{r}
#combine test and train
all75 = as.data.frame(rbind(normdf, normtest))

#use 10-fold cross-validation to find the optimal neighbors
trcontrol = trainControl(method = "cv", number = 10)
knnfit = train(class ~., method = "knn", tuneGrid = expand.grid(k = 1:60),
                trControl = trcontrol, metric = "Accuracy", data = all75)

#get the number of k  and use it in knn
kuse = knnfit$bestTune
```

The variable `kuse` is used to store the number of k that provides the highest accuracy. The following plot show the different accuracy by different number of neighbors.

```{r knnfit}
ggplot() +
  geom_line(aes(x = 1:60, y = knnfit$results[,2])) +
  labs(x = "Number of Neighbors", y = "Accuracy (Cross-Validation)", 
       title = "Accuracy by different neighbors") +
  theme_minimal()
```

Now perform knn. 

```{r}
knntrain = knn3Train(normdf[,-1], normtest[,-1], cl = normdf[,1], k = kuse, prob = T)
knntab = table(knntrain, normtest$class)
kable(knntab)
```


Accuracy:

```{r}
sum(diag(knntab))/sum(knntab)
```


#Random Forest 

```{r}
hyper_grid = expand.grid(
        mtry=c(1:15),
        node_size=c(1:15),
        sampe_size=c(0.55,0.632,0.7,0.8),
        OOB_err=0
)
```

```{r}
start.time = Sys.time()
for(i in 1:nrow(hyper_grid)) {
        ranger_rf = ranger(
                formula=class~.,
                data=df75,
                num.trees=2000,
                mtry=hyper_grid$mtry[i],
                min.node.size=hyper_grid$node_size[i],
                sample.fraction=hyper_grid$sampe_size[i],
                seed=1
        )

        hyper_grid$OOB_err[i] = ranger_rf$prediction.error
}
end.time = Sys.time()
time.taken = end.time - start.time
time.taken
```

```{r}
hyper_grid %>%
dplyr::arrange(OOB_err) %>%
head(10)
```

```{r}
which.min(hyper_grid$OOB_err) # 775

hyper_grid[775,]
```

```{r}
ranger_rf = ranger(
                formula=class~.,
                data=df75,
                num.trees=2000,
                mtry=10,
                min.node.size=7,
                sample.fraction=0.8,
                seed=1,
                importance='impurity'
        )
ranger_rf
```

```{r rftraintable}
ranger_rf$confusion.matrix
```

```{r}
imp = ranger_rf$variable.importance
```

```{r rfvarimp}
imp = as.data.frame(imp)
head(imp)
imp$varnames = rownames(imp)       
rownames(imp) = NULL
head(imp)

ggplot(imp,
       aes(x=reorder(varnames, imp),
           y=imp)) +
        geom_point() +
        geom_segment(aes(x=varnames, xend=varnames, y=0, yend=imp)) +
        ylab('Mean Decrease in Impurity') +
        xlab('Variable Name') +
        coord_flip() +
        theme_minimal()
```

```{r}
pred_RF = predict(ranger_rf, test75)

table(pred_RF$predictions, test75$class)
prop.table(table(pred_RF$predictions == test75$class))
```


#SVM

Use 10-fold CV to assess the optimal cost for the fitting process. Here, the radial basis kernel is chosen based on its better performance. 

```{r, CVsvm}
line.svm75 <- train(x=df75[,-1], y=df75[,1],
                  method = "svmRadial",
                  trControl =trainControl(method = "cv", number = 10),
                  tuneGrid =expand.grid(C =c(1:10), sigma = 0.01))
line.svm75
plot(line.svm75)
```

The final fitted model is decided using the radial basis kernel when cost is 2 and gamma is 0.01, which is confirmed in the previous CV plot. 

```{r, svm}
# Conduct the multi-class SVM on the training set
img_svm75 <- svm(x=df75[,-1], y=df75[,1], type="C-classification", 
                 kernel="radial", cost = 2, gamma = 0.01 )
summary(img_svm75)

# Prediction on test data and save the confusion matrix in svm_table
pred_75 <- predict(img_svm75, newdata=test75[,-1], type="class")
svm_table <- table(pred_75, test75$class)

# Calculate the misclassification rate
matrix=as.matrix(table(pred_75, test75$class))
diag=diag(matrix)
n=sum(matrix)
accuracy= sum(diag)/n 
accuracy
```

```{r, svmtable}
svm_table
```

